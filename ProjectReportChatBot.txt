Project Title:
Self-Hosted LLM Chat – Professional Interface

Objective:
The main objective of this project is to build a privacy-focused and customizable chat interface that runs locally using open-source large language models (LLMs). The system allows users to interact with AI models such as LLaMA 3, Mistral, or Grok-3-Mini without relying on cloud-based APIs, ensuring full data control and offline accessibility.

Description:
The Self-Hosted LLM Chat Application provides a modern and user-friendly interface to communicate with locally deployed language models through the Ollama framework. Built using Streamlit, the app delivers a seamless chat experience, preserving message history within the session and displaying AI responses dynamically. This project showcases how locally hosted models can be effectively used for personal or organizational AI solutions.

2. System Requirements

Hardware Requirements:

* Processor: Intel i5 / Ryzen 5 or higher
* RAM: Minimum 8 GB (16 GB recommended)
* Storage: At least 10 GB free space
* GPU: Optional (for faster inference, CUDA-compatible GPU preferred)

Software Requirements:

* Operating System: Windows 10 / 11, macOS, or Linux
* Python Version: 3.10 or above
* Framework: Streamlit
* Model Host: Ollama (Local Model Server)
* Additional Libraries: time, dotenv, PIL (optional)
* Browser: Chrome / Edge / Firefox for UI access


3. Tools and Technologies Used

Programming Language: Python

Framework / Library: Streamlit – used for building the interactive chat interface.

Model Integration Tool: Ollama – a local model hosting framework that allows running LLMs like LLaMA 3 or Mistral offline.

Models Used: LLaMA 3 (default), Mistral, and Grok-3-Mini (optional)

Utilities:

* time – for loading spinners and response delays
* dotenv – for managing environment variables
* PIL – for handling optional image-based features

Deployment Environment:
Localhost environment using `streamlit run main.py` command.

Version Control:
Git and GitHub used for maintaining version history and collaboration.

4. Implementation Steps

Step 1 – Environment Setup:
Installed Python and created a virtual environment.
Installed required dependencies such as Streamlit and Ollama using `pip install streamlit ollama`.
Configured Ollama by downloading and setting up the desired model using commands like `ollama pull llama3`.

Step 2 – UI Development using Streamlit:
Designed a simple, modern, and responsive interface using Streamlit components.
Configured page title, layout, and chat input section with `st.chat_input()` and `st.chat_message()`.

Step 3 – Model Integration using Ollama:
Integrated the Ollama API within the Streamlit app to connect and send chat messages to the local model.
Handled user messages and AI responses dynamically through the `ollama.chat()` function.

Step 4 – Session Management:
Used `st.session_state` to store chat history between user and AI within the same session.
Ensured that the chat history is displayed persistently during the user session.

Step 5 – Response Handling:
Added spinner effects (`st.spinner("Thinking...")`) while waiting for AI responses.
Displayed assistant messages using markdown formatting for readability.

Step 6 – Testing and Validation:
Tested the app on local environments with different models.
Validated performance, response time, and UI stability.
Fixed minor issues related to session persistence and Streamlit reloads.

5. Challenges Faced

Challenge 1: Model Loading Failure

* Issue Faced: Ollama failed to load the model initially with a “model not found” error.
* Reason / Root Cause: Model was not downloaded or configured properly in the Ollama environment.
* Rectification Step: Used `ollama pull llama3` to download the model manually and restarted the Ollama service.

Challenge 2: Streamlit Session Reset on Refresh

* Issue Faced: Chat history cleared each time the page refreshed.
* Reason / Root Cause: Session state variables were not persisted correctly.
* Rectification Step: Implemented `st.session_state` to maintain message lists and initialized them before UI rendering.

Challenge 3: Delay in Model Response

* Issue Faced: Response generation was slow on initial interaction.
* Reason / Root Cause: Model was initializing on first call and system memory usage was high.
* Rectification Step: Preloaded model during startup and reduced background processes to optimize system resources.

Challenge 4: Dependency Conflicts

* Issue Faced: Version conflicts between Streamlit and Python packages.
* Reason / Root Cause: Outdated dependency versions in environment.
* Rectification Step: Created a new virtual environment and installed dependencies with compatible versions using `pip install -r requirements.txt`.

Challenge 5: CUDA Setup Issue (GPU Mode)

* Issue Faced: Model couldn’t access GPU acceleration.
* Reason / Root Cause: Missing CUDA Toolkit or incorrect driver version.
* Rectification Step: Installed correct NVIDIA CUDA drivers and verified GPU detection using `ollama info`.

6. Testing and Validation

Unit Testing:
Verified that chat input, response handling, and UI elements worked correctly.
Checked model connectivity and message flow with different test prompts.

Performance Testing:
Measured average response times between user prompt and AI output.
Tested application behavior under continuous message exchange.

User Acceptance Testing (UAT):
Collected feedback on usability, interface clarity, and response relevance.
Refined UI spacing, font size, and spinner delays based on feedback.

7. Results and Output

The final chat application successfully enables local, private interaction with open-source models.
The Streamlit UI provides a professional look with real-time conversation flow.
Users can easily switch between models like LLaMA 3 or Mistral by editing the model name in the code.
Performance was found to be satisfactory for both CPU and GPU modes.

8. Future Enhancements

* Implement persistent chat history using a local database (SQLite or ChromaDB).
* Add user authentication and access control for multi-user environments.
* Enable multimodal capabilities (image, audio inputs).
* Integrate model selector dropdown in UI for switching models dynamically.
* Optimize GPU utilization and support model quantization for faster inference.
* Add analytics to measure model response time and user interaction metrics.

9. Conclusion

The Self-Hosted LLM Chat Application demonstrates how open-source models can be effectively deployed locally using tools like Ollama and Streamlit. The project not only highlights the ease of building an AI chat interface but also provides complete data privacy and customization.
Through this project, practical experience was gained in model integration, UI development, session management, and debugging AI responses. The end product is a stable and efficient system capable of running fully offline AI conversations.

10. References

1. Streamlit Documentation – [https://docs.streamlit.io](https://docs.streamlit.io)
2. Ollama Documentation – [https://ollama.ai](https://ollama.ai)
3. Meta LLaMA 3 Model Card – [https://ai.meta.com/llama](https://ai.meta.com/llama)
4. Python Official Site – [https://www.python.org](https://www.python.org)
5. GitHub – [https://github.com/ollama](https://github.com/ollama)


